# app.py
import io
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import streamlit as st
from sklearn.metrics import r2_score

# ---------- Industry Baselines ----------
# Numeric values are illustrative; tune them as needed for your research.
INDUSTRY_BASELINES = {
    "Assets": 5000,          # NIST/ISO 27005-style asset universe (example)
    "ThreatVectors": 200,    # MITRE threat catalog size (example)
    "Vulnerabilities": 3000, # CVSS/NVD yearly vulnerability load (example)
    "Complexity": 7,         # Gartner-style complexity score (1â€“10)
    "ChangeRate": 5,         # DORA-like change rate (1â€“10)
    "OrgMaturity": 6,        # CMMI-based maturity level (1â€“10)
    "Automation": 5,         # Gartner automation level (1â€“10)
}

# ---------- Page ----------
st.set_page_config(page_title="PASTA Effort Estimator", layout="wide")
st.title("ðŸ“Š PASTA Effort Estimator â€” Noise, Log Mode & Sensitivity")

# ---------- Sidebar: Controls ----------
with st.sidebar:
    st.header("ðŸ”§ Dataset Settings")
    num_samples = st.slider("Number of Samples", 50, 20000, 200, help="How many rows to generate.")
    scaling_factor = st.slider("Scaling Factor (S)", 0.1, 3.0, 1.1, 0.1)
    rng_seed = st.number_input("Random Seed", value=42, help="For reproducibility.")

    st.header("ðŸŽ›ï¸ Attribute Ranges")
    # Assets
    asset_range = st.slider("Assets (A)", 10, 10000, (100, 300))
    use_bmk_assets = st.checkbox("Baseline â€“ NIST/ISO 27005 (Assets)", value=False)

    # Threat Vectors
    threat_range = st.slider("Threat Vectors (T)", 1, 5000, (10, 80))
    use_bmk_threats = st.checkbox("Baseline â€“ MITRE (Threat Vectors)", value=False)

    # Vulnerabilities
    vuln_range = st.slider("Vulnerabilities (V)", 1, 20000, (30, 200))
    use_bmk_vuln = st.checkbox("Baseline â€“ CVSS Score as per NVD (Vulnerabilities)", value=False)

    # Complexity
    complexity_range = st.slider("Complexity (C)", 1, 10, (1, 5))
    use_bmk_complexity = st.checkbox("Baseline â€“ Gartner (Complexity)", value=False)

    # Change Rate
    change_rate_range = st.slider("Change Rate (R)", 1, 10, (1, 5))
    use_bmk_change = st.checkbox("Baseline â€“ DORA (Change Rate)", value=False)

    # Org Maturity
    maturity_range = st.slider("Org Maturity (M)", 1, 10, (1, 5))
    use_bmk_maturity = st.checkbox("Baseline â€“ CMMI (Org Maturity)", value=False)

    # Automation
    automation_range = st.slider("Automation (Au)", 1, 10, (1, 5))
    use_bmk_automation = st.checkbox("Baseline â€“ Gartner (Automation)", value=False)

    baseline_flags = {
        "Assets": use_bmk_assets,
        "ThreatVectors": use_bmk_threats,
        "Vulnerabilities": use_bmk_vuln,
        "Complexity": use_bmk_complexity,
        "ChangeRate": use_bmk_change,
        "OrgMaturity": use_bmk_maturity,
        "Automation": use_bmk_automation,
    }

    st.header("ðŸ”Š Noise Settings")
    noise_type = st.selectbox("Noise Distribution", ["None", "Gaussian", "Laplace"])
    noise_pct = st.slider(
        "Noise Scale (% of mean Predicted)", 0.0, 100.0, 5.0, 0.5,
        help="Standard deviation (Gaussian) or b (Laplace) as % of mean(Predicted)."
    )
    noise_seed = st.number_input("Noise Seed", value=123, help="Separate seed for noise.")

    st.header("ðŸ“ Analysis Settings")
    log_mode = st.checkbox(
        "Analyze/Plot in log10 space", value=False,
        help="RÂ² and plots computed on log10(Actual) vs log10(Predicted)."
    )
    ofat_points = st.slider("OFAT sweep points", 10, 200, 50,
                            help="Number of points in one-factor-at-a-time sweep.")
    selected_var = st.selectbox(
        "Sensitivity: variable to sweep (OFAT)",
        ["Assets", "ThreatVectors", "Vulnerabilities", "Complexity", "ChangeRate", "OrgMaturity", "Automation"]
    )

# ---------- Helpers ----------
def generate_data(n, ranges, seed, baseline_flags=None, baselines=None):
    """
    Generate synthetic data.

    If baseline_flags[var] is True, use baselines[var] as a fixed value for all rows.
    Otherwise sample uniformly from ranges[var].
    """
    np.random.seed(seed)
    baseline_flags = baseline_flags or {}
    baselines = baselines or {}

    # Assets
    if baseline_flags.get("Assets"):
        A = np.full(n, int(baselines["Assets"]), dtype=int)
    else:
        A = np.random.randint(ranges["Assets"][0], ranges["Assets"][1] + 1, n)

    # Threat Vectors
    if baseline_flags.get("ThreatVectors"):
        T = np.full(n, int(baselines["ThreatVectors"]), dtype=int)
    else:
        T = np.random.randint(ranges["ThreatVectors"][0], ranges["ThreatVectors"][1] + 1, n)

    # Vulnerabilities
    if baseline_flags.get("Vulnerabilities"):
        V = np.full(n, int(baselines["Vulnerabilities"]), dtype=int)
    else:
        V = np.random.randint(ranges["Vulnerabilities"][0], ranges["Vulnerabilities"][1] + 1, n)

    # Complexity
    if baseline_flags.get("Complexity"):
        C = np.full(n, int(baselines["Complexity"]), dtype=int)
    else:
        C = np.random.randint(ranges["Complexity"][0], ranges["Complexity"][1] + 1, n)

    # Change Rate
    if baseline_flags.get("ChangeRate"):
        R = np.full(n, int(baselines["ChangeRate"]), dtype=int)
    else:
        R = np.random.randint(ranges["ChangeRate"][0], ranges["ChangeRate"][1] + 1, n)

    # Org Maturity
    if baseline_flags.get("OrgMaturity"):
        M = np.full(n, int(baselines["OrgMaturity"]), dtype=int)
    else:
        M = np.random.randint(ranges["OrgMaturity"][0], ranges["OrgMaturity"][1] + 1, n)

    # Automation
    if baseline_flags.get("Automation"):
        Au = np.full(n, int(baselines["Automation"]), dtype=int)
    else:
        Au = np.random.randint(ranges["Automation"][0], ranges["Automation"][1] + 1, n)

    return pd.DataFrame({
        "Assets": A, "ThreatVectors": T, "Vulnerabilities": V,
        "Complexity": C, "ChangeRate": R, "OrgMaturity": M, "Automation": Au
    })

def model_predicted(df, S):
    num = df["Assets"] * df["ThreatVectors"] * df["Vulnerabilities"] * df["Complexity"] * df["ChangeRate"]
    den = df["OrgMaturity"] * df["Automation"]
    den = den.replace(0, 1)  # defensive
    return (num.astype(np.float64)) ** S / den

def add_noise(y_pred, noise_type, noise_pct, seed):
    if noise_type == "None" or noise_pct == 0.0:
        return y_pred.copy()
    rng = np.random.default_rng(seed)
    scale = (noise_pct / 100.0) * float(np.mean(y_pred))
    if scale == 0:
        return y_pred.copy()
    if noise_type == "Gaussian":
        noise = rng.normal(loc=0.0, scale=scale, size=len(y_pred))
    else:  # Laplace
        noise = rng.laplace(loc=0.0, scale=scale, size=len(y_pred))
    return y_pred + noise

def safe_log10(x):
    x = np.asarray(x, dtype=np.float64)
    min_pos = np.min(x[x > 0]) if np.any(x > 0) else 1.0
    eps = min_pos * 1e-9
    return np.log10(np.clip(x, eps, None))

def figure_to_bytes(fig):
    buf = io.BytesIO()
    fig.savefig(buf, format="png", bbox_inches="tight", dpi=180)
    buf.seek(0)
    return buf

# ---------- Generate data & targets ----------
ranges = {
    "Assets": asset_range, "ThreatVectors": threat_range, "Vulnerabilities": vuln_range,
    "Complexity": complexity_range, "ChangeRate": change_rate_range,
    "OrgMaturity": maturity_range, "Automation": automation_range,
}

df = generate_data(num_samples, ranges, seed=rng_seed, baseline_flags=baseline_flags, baselines=INDUSTRY_BASELINES)
df["PredictedEffort"] = model_predicted(df, scaling_factor)
df["ActualEffort"] = add_noise(df["PredictedEffort"], noise_type, noise_pct, seed=noise_seed)

# ---------- Metrics ----------
if log_mode:
    y_true = safe_log10(df["ActualEffort"])
    y_pred = safe_log10(df["PredictedEffort"])
    r2_log = r2_score(y_true, y_pred)
    r2_raw = r2_score(df["ActualEffort"], df["PredictedEffort"])
else:
    y_true = df["ActualEffort"].values
    y_pred = df["PredictedEffort"].values
    r2_raw = r2_score(y_true, y_pred)
    r2_log = r2_score(safe_log10(df["ActualEffort"]), safe_log10(df["PredictedEffort"]))

st.subheader("âœ… Model Validation")
cols = st.columns(2)
cols[0].metric("RÂ² (Raw)", f"{r2_raw:.4f}")
cols[1].metric("RÂ² (log10)", f"{r2_log:.4f}")
st.caption("Note: Synthetic targets (Actual = Predicted + small noise) yield optimistic RÂ²; use as a sanity check.")

# ---------- Data preview & downloads ----------
st.subheader("ðŸ“‹ Sample of Generated Dataset")
st.dataframe(df.head(20), use_container_width=True)

csv_bytes = df.to_csv(index=False).encode("utf-8")
st.download_button("ðŸ“¥ Download Dataset (CSV)", csv_bytes, file_name="pasta_effort_dataset.csv", mime="text/csv")

config = {
    "num_samples": num_samples, "scaling_factor_S": scaling_factor,
    "ranges": ranges, "rng_seed": rng_seed,
    "industry_baselines": INDUSTRY_BASELINES, "baseline_flags": baseline_flags,
    "noise": {"type": noise_type, "percent_of_mean": noise_pct, "noise_seed": noise_seed},
    "analysis": {"log_mode": log_mode, "ofat_points": ofat_points, "selected_var": selected_var},
}
st.download_button("ðŸ§¾ Download Config (JSON)", json.dumps(config, indent=2).encode("utf-8"),
                   file_name="experiment_config.json", mime="application/json")

st.markdown("---")

# ---------- Plots: Actual vs Predicted + Residuals ----------
plot_col1, plot_col2 = st.columns(2)

with plot_col1:
    st.markdown("### Actual vs Predicted")
    fig1, ax1 = plt.subplots(figsize=(6, 5))
    ax1.scatter(y_true, y_pred, alpha=0.6)
    xymin = float(min(np.min(y_true), np.min(y_pred)))
    xymax = float(max(np.max(y_true), np.max(y_pred)))
    ax1.plot([xymin, xymax], [xymin, xymax], linestyle="--")
    ax1.set_xlabel("Actual" + (" (log10)" if log_mode else ""))
    ax1.set_ylabel("Predicted" + (" (log10)" if log_mode else ""))
    ax1.set_title("Actual vs Predicted Effort")
    ax1.grid(True, alpha=0.3)
    st.pyplot(fig1, use_container_width=True)
    st.download_button("ðŸ“¤ Download Plot: Actual vs Predicted (PNG)",
                       data=figure_to_bytes(fig1),
                       file_name="actual_vs_predicted.png", mime="image/png")

with plot_col2:
    st.markdown("### Residuals")
    residuals = y_true - y_pred
    fig2, ax2 = plt.subplots(figsize=(6, 5))
    ax2.scatter(y_true, residuals, alpha=0.6)
    ax2.axhline(0.0, linestyle="--")
    ax2.set_xlabel("Actual" + (" (log10)" if log_mode else ""))
    ax2.set_ylabel("Residual (Actual - Predicted)" + (" (log10 space)" if log_mode else ""))
    ax2.set_title("Residuals vs Actual")
    ax2.grid(True, alpha=0.3)
    st.pyplot(fig2, use_container_width=True)
    st.download_button("ðŸ“¤ Download Plot: Residuals (PNG)",
                       data=figure_to_bytes(fig2),
                       file_name="residuals.png", mime="image/png")

st.markdown("---")

# ---------- Sensitivity Analysis ----------
st.subheader("ðŸ“ˆ Sensitivity Analysis (OFAT + Elasticities)")
st.caption("One-Factor-At-A-Time (OFAT): sweep one variable across its range; hold others at their medians.")

medians = {
    "Assets": int(np.median(df["Assets"])),
    "ThreatVectors": int(np.median(df["ThreatVectors"])),
    "Vulnerabilities": int(np.median(df["Vulnerabilities"])),
    "Complexity": int(np.median(df["Complexity"])),
    "ChangeRate": int(np.median(df["ChangeRate"])),
    "OrgMaturity": int(np.median(df["OrgMaturity"])),
    "Automation": int(np.median(df["Automation"])),
}

lo, hi = ranges[selected_var]
xs = np.linspace(lo, hi, ofat_points, dtype=float)

base = medians.copy()
ofat_rows = []
for v in xs:
    row = base.copy()
    if selected_var in ["Assets", "ThreatVectors", "Vulnerabilities", "OrgMaturity", "Automation"]:
        row[selected_var] = int(round(v))
    else:
        row[selected_var] = float(v)
    ofat_rows.append(row)

ofat_df = pd.DataFrame(ofat_rows)
ofat_df["PredictedEffort"] = model_predicted(ofat_df, scaling_factor)

y_ofat = np.log10(ofat_df["PredictedEffort"]) if log_mode else ofat_df["PredictedEffort"]

fig3, ax3 = plt.subplots(figsize=(7, 4.8))
ax3.plot(xs, y_ofat)
ax3.set_title(f"OFAT: Effect of {selected_var} on Predicted Effort" + (" (log10)" if log_mode else ""))
ax3.set_xlabel(f"{selected_var}")
ax3.set_ylabel("Predicted Effort" + (" (log10)" if log_mode else ""))
ax3.grid(True, alpha=0.3)
st.pyplot(fig3, use_container_width=True)
st.download_button("ðŸ“¤ Download Plot: Sensitivity (PNG)",
                   data=figure_to_bytes(fig3),
                   file_name=f"sensitivity_{selected_var}.png", mime="image/png")

# ---------- Elasticities ----------
st.markdown("#### Elasticities at Median Operating Point")
st.caption("Closed-form (from the model) and empirical (finite difference) elasticities.")

closed_form = {
    "Assets": scaling_factor, "ThreatVectors": scaling_factor, "Vulnerabilities": scaling_factor,
    "Complexity": scaling_factor, "ChangeRate": scaling_factor,
    "OrgMaturity": -1.0, "Automation": -1.0
}

def empirical_elasticity(var, base_row, rel_delta=0.05):
    x0 = max(1e-9, float(base_row[var]))
    step = max(1.0, abs(x0 * rel_delta)) if var in ["Assets","ThreatVectors","Vulnerabilities","OrgMaturity","Automation"] else x0 * rel_delta
    row0, rowp = base_row.copy(), base_row.copy()
    rowp[var] = type(x0)(x0 + step)

    E0 = float(model_predicted(pd.DataFrame([row0]), scaling_factor).iloc[0])
    Ep = float(model_predicted(pd.DataFrame([rowp]), scaling_factor).iloc[0])

    dE_over_E = (Ep - E0) / max(E0, 1e-12)
    dX_over_X = (rowp[var] - x0) / max(x0, 1e-12)
    return dE_over_E / dX_over_X if dX_over_X != 0 else np.nan

empirical = {v: empirical_elasticity(v, medians) for v in closed_form.keys()}

elas_df = pd.DataFrame({
    "Variable": list(closed_form.keys()),
    "ClosedForm_Elasticity": list(closed_form.values()),
    "Empirical_Elasticity": [empirical[v] for v in closed_form.keys()]
})
st.dataframe(elas_df, use_container_width=True)
st.download_button("ðŸ“¥ Download Elasticities (CSV)", elas_df.to_csv(index=False).encode("utf-8"),
                   "elasticities.csv", "text/csv")

st.markdown("---")

st.markdown(r"""
**Model Formula**  
\[
\hat{E} = \frac{(A \times T \times V \times C \times R)^{S}}{M \times Au}
\]
where  
- \(A\)=Assets, \(T\)=ThreatVectors, \(V\)=Vulnerabilities, \(C\)=Complexity, \(R\)=ChangeRate,  
- \(M\)=OrgMaturity, \(Au\)=Automation, \(S\)=Scaling factor.
""")
